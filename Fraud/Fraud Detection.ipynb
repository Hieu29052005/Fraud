{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac7891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"hmeq-1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65500579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f56730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, recall_score, precision_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas.core.algorithms as algos\n",
    "import scipy.stats.stats as stats\n",
    "import traceback\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['BAD']].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percent = df.isnull().mean().sort_values(ascending=False) * 100\n",
    "print(missing_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a2ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [col for col in df.columns if df[col].dtype == 'object']\n",
    "numerical_features = [col for col in df.columns if df[col].dtype != 'object']\n",
    "\n",
    "print('Number of categorical features:', len(categorical_features))\n",
    "print('Number of numerical features:', len(numerical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·∫øm t·∫ßn s·ªë c·ªßa t·ª´ng gi√° tr·ªã trong m·ªói feature theo BAD\n",
    "for feature in categorical_features:\n",
    "    print(f\"\\nT·∫ßn s·ªë c·ªßa {feature} theo BAD:\")\n",
    "    freq_table = pd.crosstab(df[feature], df['BAD'], dropna=False)  # Gi·ªØ gi√° tr·ªã NaN n·∫øu c√≥\n",
    "    print(freq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['REASON'] = df['REASON'].fillna('DebtCon')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed4e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['JOB'].isnull()) & (df['BAD'] == 0), 'JOB'] = 'Office'\n",
    "df.loc[(df['JOB'].isnull()) & (df['BAD'] == 1), 'JOB'] = 'Other'\n",
    "\n",
    "for feature in categorical_features:\n",
    "    print(f\"\\nT·∫ßn s·ªë c·ªßa {feature} theo BAD:\")\n",
    "    freq_table = pd.crosstab(df[feature], df['BAD'], dropna=False)  # Gi·ªØ gi√° tr·ªã NaN n·∫øu c√≥\n",
    "    print(freq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f09334",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value_bad_0 = df[df['BAD'] == 0]['VALUE'].median()\n",
    "median_value_bad_1 = df[df['BAD'] == 1]['VALUE'].median()\n",
    "\n",
    "# ƒêi·ªÅn gi√° tr·ªã thi·∫øu theo BAD\n",
    "df.loc[(df['VALUE'].isna()) & (df['BAD'] == 0), 'VALUE'] = median_value_bad_0\n",
    "df.loc[(df['VALUE'].isna()) & (df['BAD'] == 1), 'VALUE'] = median_value_bad_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_median_by_bad = ['CLNO', 'YOJ', 'CLAGE', 'NINQ', 'DELINQ', 'DEROG']\n",
    "for col in cols_median_by_bad:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    median_bad_0 = df[df['BAD'] == 0][col].median()\n",
    "    median_bad_1 = df[df['BAD'] == 1][col].median()\n",
    "    df.loc[(df[col].isna()) & (df['BAD'] == 0), col] = median_bad_0\n",
    "    df.loc[(df[col].isna()) & (df['BAD'] == 1), col] = median_bad_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mortdue = ['CLNO', 'VALUE', 'LOAN', 'BAD', 'MORTDUE']\n",
    "df_mortdue_impute = df[features_mortdue].copy()\n",
    "\n",
    "# ƒê·∫£m b·∫£o ch·ªâ s·ªë ƒë·ªìng b·ªô\n",
    "df_mortdue_impute.index = df.index\n",
    "\n",
    "# X·ª≠ l√Ω gi√° tr·ªã kh√¥ng ph·∫£i s·ªë\n",
    "for col in features_mortdue:\n",
    "    df_mortdue_impute[col] = pd.to_numeric(df_mortdue_impute[col], errors='coerce')\n",
    "\n",
    "# Ki·ªÉm tra NaN tr∆∞·ªõc khi ch·∫°y KNN\n",
    "print(\"\\nS·ªë l∆∞·ª£ng NaN trong c√°c feature tr∆∞·ªõc khi ƒëi·ªÅn MORTDUE:\")\n",
    "print(df_mortdue_impute.isna().sum())\n",
    "\n",
    "# √Åp d·ª•ng KNN Imputer\n",
    "imputer_mortdue = KNNImputer(n_neighbors=5)\n",
    "df_mortdue_filled = imputer_mortdue.fit_transform(df_mortdue_impute)\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi ng∆∞·ª£c v·ªÅ DataFrame\n",
    "df_mortdue_filled = pd.DataFrame(df_mortdue_filled, columns=features_mortdue, index=df.index)\n",
    "\n",
    "# G√°n l·∫°i gi√° tr·ªã MORTDUE\n",
    "df['MORTDUE'] = df_mortdue_filled['MORTDUE']\n",
    "\n",
    "# 4. Ki·ªÉm tra s·ªë l∆∞·ª£ng gi√° tr·ªã thi·∫øu sau khi ƒëi·ªÅn\n",
    "print(\"\\nS·ªë l∆∞·ª£ng gi√° tr·ªã thi·∫øu trong MORTDUE sau khi ƒëi·ªÅn:\", df['MORTDUE'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f13b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ƒêi·ªÅn DEBTINC b·∫±ng KNN Imputation\n",
    "features_debtinc = ['CLNO', 'MORTDUE', 'NINQ', 'LOAN', 'VALUE', 'BAD', 'DEBTINC']\n",
    "df_debtinc_impute = df[features_debtinc].copy()\n",
    "\n",
    "# ƒê·∫£m b·∫£o ch·ªâ s·ªë ƒë·ªìng b·ªô\n",
    "df_debtinc_impute.index = df.index\n",
    "\n",
    "# X·ª≠ l√Ω gi√° tr·ªã kh√¥ng ph·∫£i s·ªë\n",
    "for col in features_debtinc:\n",
    "    df_debtinc_impute[col] = pd.to_numeric(df_debtinc_impute[col], errors='coerce')\n",
    "\n",
    "# Ki·ªÉm tra NaN tr∆∞·ªõc khi ch·∫°y KNN\n",
    "print(\"\\nS·ªë l∆∞·ª£ng NaN trong c√°c feature tr∆∞·ªõc khi ƒëi·ªÅn DEBTINC:\")\n",
    "print(df_debtinc_impute.isna().sum())\n",
    "\n",
    "# √Åp d·ª•ng KNN Imputer\n",
    "imputer_debtinc = KNNImputer(n_neighbors=5)\n",
    "df_debtinc_filled = imputer_debtinc.fit_transform(df_debtinc_impute)\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi ng∆∞·ª£c v·ªÅ DataFrame\n",
    "df_debtinc_filled = pd.DataFrame(df_debtinc_filled, columns=features_debtinc, index=df.index)\n",
    "\n",
    "# G√°n l·∫°i gi√° tr·ªã DEBTINC\n",
    "df['DEBTINC'] = df_debtinc_filled['DEBTINC']\n",
    "\n",
    "# 4. Ki·ªÉm tra s·ªë l∆∞·ª£ng gi√° tr·ªã thi·∫øu sau khi ƒëi·ªÅn\n",
    "print(\"\\nS·ªë l∆∞·ª£ng gi√° tr·ªã thi·∫øu trong DEBTINC sau khi ƒëi·ªÅn:\", df['DEBTINC'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78879ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≠nh ma tr·∫≠n t∆∞∆°ng quan\n",
    "# Ch·ªçn ch·ªâ c√°c c·ªôt s·ªë ƒë·ªÉ t√≠nh to√°n ma tr·∫≠n t∆∞∆°ng quan\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "\n",
    "# V·∫Ω heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Heatmap of Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16922c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L·ªçc to√†n b·ªô outlier theo IQR cho c√°c bi·∫øn s·ªë\n",
    "def remove_outliers_iqr(df_input, columns):\n",
    "    df_temp = df_input.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_temp[col].quantile(0.25)\n",
    "        Q3 = df_temp[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_temp = df_temp[(df_temp[col] >= lower_bound) & (df_temp[col] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# L·∫•y danh s√°ch c√°c c·ªôt s·ªë (tr·ª´ c·ªôt target)\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns.drop('BAD')\n",
    "\n",
    "# √Åp d·ª•ng l·ªçc outlier\n",
    "df_clean = remove_outliers_iqr(df, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8bba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_woe_iv(df, feature, target, bins=10):\n",
    "    # Chia bin n·∫øu l√† continuous feature\n",
    "    if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "        df['bucket'] = pd.qcut(df[feature], q=bins, duplicates='drop')\n",
    "    else:\n",
    "        df['bucket'] = df[feature]\n",
    "\n",
    "    # T·∫°o b·∫£ng t·ªïng h·ª£p\n",
    "    woe_df = df.groupby('bucket').agg({target: ['sum', 'count']})\n",
    "    woe_df.columns = ['Bad', 'Total']\n",
    "    woe_df['Good'] = woe_df['Total'] - woe_df['Bad']\n",
    "\n",
    "    # T√≠nh t·ª∑ l·ªá % Good v√† % Bad\n",
    "    woe_df['% Good'] = woe_df['Good'] / woe_df['Good'].sum()\n",
    "    woe_df['% Bad'] = woe_df['Bad'] / woe_df['Bad'].sum()\n",
    "\n",
    "    # T√≠nh WOE v√† IV\n",
    "    woe_df['WOE'] = np.log(woe_df['% Good'] / woe_df['% Bad'])\n",
    "    woe_df['IV'] = (woe_df['% Good'] - woe_df['% Bad']) * woe_df['WOE']\n",
    "\n",
    "    # T·ªïng IV cho to√†n b·ªô feature\n",
    "    iv = woe_df['IV'].sum()\n",
    "\n",
    "    return iv, woe_df[['Bad', 'Good', 'Total', '% Good', '% Bad', 'WOE', 'IV']]\n",
    "\n",
    "# Danh s√°ch c√°c c·ªôt c·∫ßn t√≠nh IV\n",
    "features = [\"DEBTINC\", \"MORTDUE\", \"VALUE\", \"DEROG\", \"CLAGE\", \"NINQ\", \"DELINQ\", \"YOJ\", \"CLNO\", \"REASON\", \"JOB\"]\n",
    "iv_scores = {}\n",
    "\n",
    "# T√≠nh IV cho t·ª´ng feature v√† l∆∞u k·∫øt qu·∫£\n",
    "for feature in features:\n",
    "    # Check if feature exists in the dataframe before calculating IV\n",
    "    if feature in df.columns:\n",
    "        iv, detail = calculate_woe_iv(df, feature, \"BAD\")\n",
    "        # Only store and print if IV calculation was successful (iv is not 0 and detail is not None)\n",
    "        if detail is not None:\n",
    "            iv_scores[feature] = iv\n",
    "            print(f\"Feature: {feature}, IV: {iv:.4f}\")\n",
    "        else:\n",
    "            print(f\"Feature: {feature}, IV cannot be calculated (all target values are the same or missing).\")\n",
    "    else:\n",
    "        print(f\"Feature: {feature} not found in DataFrame.\")\n",
    "\n",
    "\n",
    "# S·∫Øp x·∫øp c√°c feature theo IV t·ª´ cao xu·ªëng th·∫•p\n",
    "# Convert the dictionary to a list of tuples for sorting\n",
    "iv_sorted = sorted(iv_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# In k·∫øt qu·∫£\n",
    "print(\"\\nSorted IV scores:\") # This line is corrected\n",
    "for feature, iv in iv_sorted:\n",
    "    print(f\"{feature}: {iv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16021e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√≥a c·ªôt 'REASON'\n",
    "df = df.drop('REASON', axis=1)\n",
    "df = df.drop('bucket', axis=1)\n",
    "\n",
    "# Ki·ªÉm tra l·∫°i c√°c c·ªôt sau khi x√≥a\n",
    "print(\"C√°c c·ªôt sau khi x√≥a 'REASON':\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒêi·ªÅn missing value tr∆∞·ªõc khi encode\n",
    "df[\"JOB\"] = df[\"JOB\"].fillna(\"Unknown\")\n",
    "\n",
    "# Th·ª±c hi·ªán One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=[\"JOB\"], drop_first=True)\n",
    "\n",
    "# Ki·ªÉm tra k·∫øt qu·∫£\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.filter(like=\"JOB_\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuy·ªÉn ƒë·ªïi t·∫•t c·∫£ c√°c c·ªôt JOB_* t·ª´ boolean v·ªÅ integer\n",
    "job_cols = df.filter(like=\"JOB_\").columns\n",
    "df[job_cols] = df[job_cols].astype(int)\n",
    "# Ki·ªÉm tra l·∫°i\n",
    "print(df[job_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f80b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e2297",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"BAD\"])\n",
    "y = df[\"BAD\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu c·ªßa c√°c c·ªôt\n",
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b055be",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "cols = X_train.columns\n",
    "X_train[cols] = scaler.fit_transform(X_train[cols])\n",
    "X_test[cols] = scaler.transform(X_test[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train,y_train = sm.fit_resample(X_train,y_train)\n",
    "print(\"Dimension of X_train_sm Shape:\", X_train.shape)\n",
    "print(\"Dimension of y_train_sm Shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d75e6",
   "metadata": {},
   "source": [
    "1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38930ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l1', C=0.9, solver='saga', n_jobs=-1)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"Accuracy of TEST data: {:.2f}%\".format(100 * accuracy_score(y_test, y_test_pred)))\n",
    "    print(\"F1 Score of TEST data: {:.2f}%\".format(100 * f1_score(y_test, y_test_pred, average=\"macro\")))\n",
    "    print(\"Recall of TEST data: {:.2f}%\".format(100 * recall_score(y_test, y_test_pred, average=\"macro\")))\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    print(\"RMSE: \", rmse)\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_test_pred, average=None)\n",
    "    print(\"ROC AUC score: \", roc_auc)\n",
    "\n",
    "evaluation(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f23e83",
   "metadata": {},
   "source": [
    "2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_rfr = RandomForestClassifier(random_state=42,oob_score=True)\n",
    "regr_rfr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(regr_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1200d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, recall_score, classification_report, roc_auc_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# === B∆∞·ªõc 1: H√†m Recall t√πy ch·ªânh cho l·ªõp 1 (BAD) ===\n",
    "def custom_recall_score(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "recall_scorer = make_scorer(custom_recall_score)\n",
    "\n",
    "# === B∆∞·ªõc 2: H√†m objective cho Optuna ===\n",
    "def objective(trial):\n",
    "    # T·∫≠p si√™u tham s·ªë cho RandomForest\n",
    "    params = {\n",
    "        \"classifier__n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "        \"classifier__max_depth\": trial.suggest_int(\"max_depth\", 2, 22),\n",
    "        \"classifier__min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"classifier__min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "        \"classifier__max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "        \"classifier__bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "        \"classifier__ccp_alpha\": trial.suggest_float(\"ccp_alpha\", 0.0, 0.1)\n",
    "    }\n",
    "\n",
    "    # T·∫°o pipeline v·ªõi SMOTE v√† RandomForestClassifier\n",
    "    pipeline = ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    pipeline.set_params(**params)\n",
    "\n",
    "    # T√≠nh ƒëi·ªÉm Recall trung b√¨nh t·ª´ 5-fold cross-validation\n",
    "    recall = cross_val_score(pipeline, X_train, y_train, scoring=recall_scorer, cv=5).mean()\n",
    "    return recall\n",
    "\n",
    "# === B∆∞·ªõc 3: T·∫°o v√† ch·∫°y Optuna study ===\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=500, timeout=600)\n",
    "\n",
    "# === B∆∞·ªõc 4: In k·∫øt qu·∫£ t·ªët nh·∫•t ===\n",
    "print(\"‚úÖ Best Recall (train CV):\", study.best_value)\n",
    "print(\"üìå Best hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# === B∆∞·ªõc 5: Hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh v·ªõi c√°c si√™u tham s·ªë t·ªët nh·∫•t ===\n",
    "best_params = {key.replace(\"classifier__\", \"\"): value for key, value in study.best_params.items()}\n",
    "best_rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "\n",
    "# T·∫°o pipeline cho hu·∫•n luy·ªán cu·ªëi c√πng\n",
    "final_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', best_rf_model)\n",
    "])\n",
    "\n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n to√†n b·ªô t·∫≠p train\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === B∆∞·ªõc 6: H√†m evaluation t√πy ch·ªânh ===\n",
    "def evaluation(model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # D·ª± ƒëo√°n x√°c su·∫•t cho ROC AUC\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "\n",
    "    # In k·∫øt qu·∫£ cho t·∫≠p test\n",
    "    print(\"\\n==================================================\")\n",
    "    print(\"Evaluation for Test Data\")\n",
    "    print(\"==================================================\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(f\"Accuracy of TEST data: {100 * np.mean(y_test_pred == y_test):.2f}\")\n",
    "    print(f\"Recall of TEST data (class 1): {100 * recall_score(y_test, y_test_pred, pos_label=1):.2f}\")\n",
    "\n",
    "    # ROC AUC score (n·∫øu c√≥ x√°c su·∫•t)\n",
    "    if y_test_proba is not None:\n",
    "        print(f\"ROC AUC score: {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
    "\n",
    "    # RMSE (d·ª±a tr√™n x√°c su·∫•t, n·∫øu c√≥)\n",
    "    if y_test_proba is not None:\n",
    "        rmse = np.sqrt(np.mean((y_test - y_test_proba) ** 2))\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# === B∆∞·ªõc 7: ƒê√°nh gi√° m√¥ h√¨nh t·ªët nh·∫•t ===\n",
    "print(\"\\n==================================================\")\n",
    "print(\"Evaluation for the best Random Forest Model\")\n",
    "evaluation(final_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62235ea7",
   "metadata": {},
   "source": [
    "3. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213833d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59626e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, recall_score, classification_report, roc_auc_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    " \n",
    "# Gi·∫£ ƒë·ªãnh X_train, y_train, X_test, y_test ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n",
    "# N·∫øu ch∆∞a c√≥, b·∫°n c·∫ßn th√™m ƒëo·∫°n m√£ chia d·ªØ li·ªáu:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "# === B∆∞·ªõc 1: H√†m Recall t√πy ch·ªânh cho l·ªõp 1 (BAD) ===\n",
    "def custom_recall_score(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=1)\n",
    " \n",
    "recall_scorer = make_scorer(custom_recall_score)\n",
    " \n",
    "# === B∆∞·ªõc 2: H√†m objective cho Optuna ===\n",
    "def objective(trial):\n",
    "    # T·∫≠p si√™u tham s·ªë c·∫ßn t√¨m (gi·ªõi h·∫°n max_depth ƒë·ªÉ gi·∫£m overfitting)\n",
    "    params = {\n",
    "        \"classifier__criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "        \"classifier__max_depth\": trial.suggest_int(\"max_depth\", 2, 22),  # Gi·∫£m t·ª´ 40 xu·ªëng 20\n",
    "        \"classifier__min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"classifier__min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
    "        \"classifier__max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "        \"classifier__ccp_alpha\": trial.suggest_float(\"ccp_alpha\", 0.0, 0.1)  # Th√™m ccp_alpha\n",
    "    }\n",
    " \n",
    "    # T·∫°o pipeline v·ªõi SMOTE v√† DecisionTreeClassifier\n",
    "    pipeline = ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "    ])\n",
    "    pipeline.set_params(**params)\n",
    " \n",
    "    # T√≠nh ƒëi·ªÉm Recall trung b√¨nh t·ª´ 5-fold cross-validation\n",
    "    recall = cross_val_score(pipeline, X_train, y_train, scoring=recall_scorer, cv=5).mean()\n",
    "    return recall\n",
    " \n",
    "# === B∆∞·ªõc 3: T·∫°o v√† ch·∫°y Optuna study ===\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=500, timeout=600)  # Gi·∫£m n_trials t·ª´ 20000 xu·ªëng 100\n",
    " \n",
    "# === B∆∞·ªõc 4: In k·∫øt qu·∫£ t·ªët nh·∫•t ===\n",
    "print(\"‚úÖ Best Recall (train CV):\", study.best_value)\n",
    "print(\"üìå Best hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    " \n",
    "# === B∆∞·ªõc 5: Hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh v·ªõi c√°c si√™u tham s·ªë t·ªët nh·∫•t ===\n",
    "best_params = {key.replace(\"classifier__\", \"\"): value for key, value in study.best_params.items()}\n",
    "best_tree_model = DecisionTreeClassifier(**best_params, random_state=42)\n",
    " \n",
    "# T·∫°o pipeline cho hu·∫•n luy·ªán cu·ªëi c√πng\n",
    "final_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', best_tree_model)\n",
    "])\n",
    " \n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n to√†n b·ªô t·∫≠p train\n",
    "final_pipeline.fit(X_train, y_train)\n",
    " \n",
    "# === B∆∞·ªõc 6: H√†m evaluation t√πy ch·ªânh ===\n",
    "def evaluation(model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):\n",
    "    # D·ª± ƒëo√°n tr√™n t·∫≠p train v√† test\n",
    "    y_test_pred = model.predict(X_test)\n",
    " \n",
    "    # D·ª± ƒëo√°n x√°c su·∫•t cho ROC AUC\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    " \n",
    "    # In k·∫øt qu·∫£ cho t·∫≠p test\n",
    "    print(\"\\n==================================================\")\n",
    "    print(\"Evaluation for Test Data\")\n",
    "    print(\"==================================================\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(f\"Accuracy of TEST data: {100 * np.mean(y_test_pred == y_test):.2f}\")\n",
    "    print(f\"Recall of TEST data (class 1): {100 * recall_score(y_test, y_test_pred, pos_label=1):.2f}\")\n",
    " \n",
    "    # ROC AUC score (n·∫øu c√≥ x√°c su·∫•t)\n",
    "    if y_test_proba is not None:\n",
    "        print(f\"ROC AUC score: {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
    " \n",
    "    # RMSE (d·ª±a tr√™n x√°c su·∫•t, n·∫øu c√≥)\n",
    "    if y_test_proba is not None:\n",
    "        rmse = np.sqrt(np.mean((y_test - y_test_proba) ** 2))\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    " \n",
    "# === B∆∞·ªõc 7: ƒê√°nh gi√° m√¥ h√¨nh t·ªët nh·∫•t ===\n",
    "print(\"\\n==================================================\")\n",
    "print(\"Evaluation for the best Decision Tree Model\")\n",
    "evaluation(final_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c466b2",
   "metadata": {},
   "source": [
    "4. CatBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e343802",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    " \n",
    "\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    eval_metric='AUC',\n",
    "    random_seed=42\n",
    ")\n",
    " \n",
    "cat_model.fit(X_train, y_train, early_stopping_rounds=50)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad137313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # N·∫øu ƒë·∫ßu ra l√† x√°c su·∫•t (continuous), c·∫ßn chuy·ªÉn sang nh√£n (binary)\n",
    "    if y_test_pred.ndim == 2 and y_test_pred.shape[1] == 1:\n",
    "        y_test_pred = y_test_pred.ravel()  # flatten if shape is (n_samples, 1)\n",
    "\n",
    "    y_test_pred_class = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "    print(classification_report(y_test, y_test_pred_class))\n",
    "    print(\"Accuracy of TEST data: {:.2f}%\".format(100 * accuracy_score(y_test, y_test_pred_class)))\n",
    "    print(\"F1 Score of TEST data: {:.2f}%\".format(100 * f1_score(y_test, y_test_pred_class, average=\"macro\")))\n",
    "    print(\"Recall of TEST data: {:.2f}%\".format(100 * recall_score(y_test, y_test_pred_class, average=\"macro\")))\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    print(\"RMSE: \", rmse)\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    print(\"ROC AUC score: \", roc_auc)\n",
    "\n",
    "evaluation(cat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21339432",
   "metadata": {},
   "source": [
    "5. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab17fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh LightGBM\n",
    "light_model = LGBMClassifier(random_state=42)\n",
    "light_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29633962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # N·∫øu ƒë·∫ßu ra l√† x√°c su·∫•t (continuous), c·∫ßn chuy·ªÉn sang nh√£n (binary)\n",
    "    if y_test_pred.ndim == 2 and y_test_pred.shape[1] == 1:\n",
    "        y_test_pred = y_test_pred.ravel()  # flatten if shape is (n_samples, 1)\n",
    "\n",
    "    y_test_pred_class = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "    print(classification_report(y_test, y_test_pred_class))\n",
    "    print(\"Accuracy of TEST data: {:.2f}%\".format(100 * accuracy_score(y_test, y_test_pred_class)))\n",
    "    print(\"F1 Score of TEST data: {:.2f}%\".format(100 * f1_score(y_test, y_test_pred_class, average=\"macro\")))\n",
    "    print(\"Recall of TEST data: {:.2f}%\".format(100 * recall_score(y_test, y_test_pred_class, average=\"macro\")))\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    print(\"RMSE: \", rmse)\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    print(\"ROC AUC score: \", roc_auc)\n",
    "\n",
    "evaluation(light_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52038738",
   "metadata": {},
   "source": [
    "6. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_model = XGBClassifier(use_label_encoder=False,eval_metric='logloss',random_state=42)\n",
    "x_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad136ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # N·∫øu ƒë·∫ßu ra l√† x√°c su·∫•t (continuous), c·∫ßn chuy·ªÉn sang nh√£n (binary)\n",
    "    if y_test_pred.ndim == 2 and y_test_pred.shape[1] == 1:\n",
    "        y_test_pred = y_test_pred.ravel()  # flatten if shape is (n_samples, 1)\n",
    "\n",
    "    y_test_pred_class = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "    print(classification_report(y_test, y_test_pred_class))\n",
    "    print(\"Accuracy of TEST data: {:.2f}%\".format(100 * accuracy_score(y_test, y_test_pred_class)))\n",
    "    print(\"F1 Score of TEST data: {:.2f}%\".format(100 * f1_score(y_test, y_test_pred_class, average=\"macro\")))\n",
    "    print(\"Recall of TEST data: {:.2f}%\".format(100 * recall_score(y_test, y_test_pred_class, average=\"macro\")))\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    print(\"RMSE: \", rmse)\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    print(\"ROC AUC score: \", roc_auc)\n",
    "\n",
    "evaluation(x_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25253eef",
   "metadata": {},
   "source": [
    "7. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Kh·ªüi t·∫°o c√°c k·ªπ thu·∫≠t resampling\n",
    "sampling_strategy = {0: 2384, 1:2384}\n",
    "over = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "under = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "pipeline = Pipeline(steps=[('over', over), ('under', under)])\n",
    "\n",
    "# √Åp d·ª•ng k·ªπ thu·∫≠t resampling v√†o t·∫≠p hu·∫•n luy·ªán\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "new_train_data = pd.concat([X_train_resampled, y_train_resampled], axis=1)\n",
    "print(new_train_data[['BAD']].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e4238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Chu·∫©n h√≥a d·ªØ li·ªáu (ƒê∆∞a v·ªÅ mean = 0, variance = 1)\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)  # x <--   (x - mean(x_train))/std(x_train)  (ƒê·∫£m b·∫£o t·∫≠p test ƒë∆∞·ª£c chu·∫©n h√≥a nh∆∞ t·∫≠p train.)\n",
    "\n",
    "# Hu·∫•n luy·ªán SVM v·ªõi gamma = 1\n",
    "svm_rbf = SVC(kernel='rbf', C=2, gamma=1, class_weight='balanced',probability=True, random_state=42)\n",
    "svm_rbf.fit(X_train_resampled_scaled, y_train_resampled)\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n t·∫≠p train (sau resampling)\n",
    "y_train_pred = svm_rbf.predict(X_train_resampled_scaled)\n",
    "print(\"The evaluation of training set (using SMOTETomek):\")\n",
    "print(classification_report(y_train_resampled, y_train_pred))\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "y_test_pred = svm_rbf.predict(X_test_scaled)\n",
    "print(\"\\nThe evaluation of test set:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "auc_train = roc_auc_score(y_train_resampled, y_train_pred)\n",
    "auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"AUC score tr√™n t·∫≠p train: {auc_train:.4f}\")\n",
    "print(f\"AUC score tr√™n t·∫≠p test: {auc_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a777a",
   "metadata": {},
   "source": [
    "8. Compare these model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(penalty='l1', C=0.9, solver='saga', n_jobs=-1),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators = 266,max_depth = 9, min_samples_split = 2, min_samples_leaf = 4,max_features='sqrt', bootstrap = False, ccp_alpha = 0.0002841158182021458),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False,eval_metric='logloss',random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(criterion = 'gini', max_depth = 21, min_samples_split = 6, min_samples_leaf = 1, max_features = None, ccp_alpha = 0.00015159440001651534),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=500,learning_rate=0.05,depth=6,eval_metric='AUC',random_seed=42),\n",
    "    \"SVM\": SVC(kernel='rbf', C=2, gamma=1, class_weight='balanced',probability=True, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name,model,X_train,y_train,X_test,y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test,y_pred),\n",
    "        \"Precision\": precision_score(y_test,y_pred),\n",
    "        \"Recall\": recall_score(y_test,y_pred),\n",
    "        \"F1 Score\": f1_score(y_test,y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test,y_proba)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd94964",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"Training: {name}\")\n",
    "    res = evaluate_model(name,model,X_train,y_train,X_test,y_test)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f7f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).sort_values(by=\"AUC\",ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"Accuracy\", y=\"Model\", data=results_df, palette=\"coolwarm\")\n",
    "plt.title(\"Model Performance (AUC)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c846e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 11. Save best model and preprocessor ===\n",
    "import joblib\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the best model based on AUC\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Re-fit the best model (to ensure it's trained with all data)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# === 12. Prediction function for new samples ===\n",
    "def predict_sample(new_data):\n",
    "    \"\"\"\n",
    "    Predict using the saved model and scaler.\n",
    "    \n",
    "    Args:\n",
    "        new_data (DataFrame): New data with same structure as training data\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (prediction, probability)\n",
    "    \"\"\"\n",
    "    # Load model and scaler\n",
    "    model = joblib.load('best_model.pkl')\n",
    "    scaler = joblib.load('scaler.pkl')\n",
    "    \n",
    "    # Preprocess the new data\n",
    "    # 1. Handle categoricals (JOB_* columns should already exist if using get_dummies)\n",
    "    # 2. Ensure column order matches training\n",
    "    new_data = new_data[X_train.columns]  # Reorder columns\n",
    "    \n",
    "    # Scale numerical features\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(new_data_scaled)\n",
    "    probability = model.predict_proba(new_data_scaled)[:, 1]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# === 13. Example usage ===\n",
    "# Create sample data (with all features including dummy columns)\n",
    "sample_data = pd.DataFrame({\n",
    "    'LOAN': [10000],\n",
    "    'MORTDUE': [20000],\n",
    "    'VALUE': [25000],\n",
    "    'DEROG': [0],\n",
    "    'DELINQ': [0],\n",
    "    'CLAGE': [100],\n",
    "    'NINQ': [1],\n",
    "    'CLNO': [5],\n",
    "    'DEBTINC': [35],\n",
    "    'YOJ': [5],\n",
    "    'JOB_Office': [0],\n",
    "    'JOB_Other': [1],\n",
    "    'JOB_ProfExe': [0],\n",
    "    'JOB_Sales': [0],\n",
    "    'JOB_Self': [0]\n",
    "}, index=[0])\n",
    "\n",
    "# Predict\n",
    "prediction, probability = predict_sample(sample_data)\n",
    "print(f\"\\nPrediction: {'BAD' if prediction[0] == 1 else 'GOOD'}\")\n",
    "print(f\"Probability: {probability[0]:.2f}\")\n",
    "\n",
    "# === 14. Detailed evaluation of best model ===\n",
    "print(\"\\n=== Best Model Evaluation ===\")\n",
    "print(f\"Model: {best_model_name}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nAUC-ROC:\", roc_auc_score(y_test, y_proba))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
